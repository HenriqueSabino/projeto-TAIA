{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henriquesabino/miniconda3/lib/python3.11/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
      "2023-09-03 12:17:04.297334: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-03 12:17:04.706007: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-03 12:17:05.834801: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/henriquesabino/miniconda3/lib/python3.11/site-packages/tensorflow/python/debug/cli/debugger_cli_common.py:19: DeprecationWarning: module 'sre_constants' is deprecated\n",
      "  import sre_constants\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import shutil\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from model_utils import createOrLoadModel\n",
    "\n",
    "def train_ppo(env: gym.Env, subfolder, learning_rate, clip_range):\n",
    "    model, steps_done = createOrLoadModel('ppo', env, 'ppo_', '_steps.zip', subfolder, learning_rate=learning_rate, clip_range=clip_range)\n",
    "\n",
    "    checkpoint_callback = CheckpointCallback(save_freq=50000, save_path=f'./models/{subfolder}', name_prefix='ppo')\n",
    "\n",
    "    callbacks=[checkpoint_callback]\n",
    "    training_steps = int(1e6)\n",
    "    model = model.learn(total_timesteps=(training_steps - steps_done), log_interval=4, callback=callbacks)\n",
    "\n",
    "    shutil.rmtree(f'./models/{subfolder}')\n",
    "    model.save(f'./models/{subfolder}/ppo_{training_steps}_steps.zip')\n",
    "\n",
    "    sum_rewards = 0\n",
    "    for _ in range(1000):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _ = env.step(int(action))\n",
    "            sum_rewards += reward\n",
    "\n",
    "    return sum_rewards / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import gym    \n",
    "import slimevolleygym\n",
    "from slimevolleygym import SurvivalRewardEnv\n",
    "import numpy as np\n",
    "from gym.wrappers.gray_scale_observation import GrayScaleObservation\n",
    "from gym.wrappers.resize_observation import ResizeObservation\n",
    "from atari_wrappers import RenderWrapper, BufferWrapper, ImageToPyTorch\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def optimize_ppo(trial : optuna.Trial):\n",
    "    env_name = 'SlimeVolleyNoFrameskip-v0'\n",
    "    env = gym.make(env_name)\n",
    "    env = SurvivalRewardEnv(env)\n",
    "    env = RenderWrapper(env)\n",
    "    env = ResizeObservation(env, (84, 84))\n",
    "    env = GrayScaleObservation(env, True)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4, np.uint8)\n",
    "\n",
    "    lr = trial.suggest_float ('learning_rate', 0.0001, 0.001)\n",
    "    clip_range = trial.suggest_float ('clip_range', 0.1, 0.5)\n",
    "\n",
    "    trial_date = trial.datetime_start.isoformat(timespec='seconds')\n",
    "    mean_reward = train_ppo(env, f'ppo_train_{trial_date}', lr, clip_range)\n",
    "\n",
    "    # limpa a saída da célula do notebook\n",
    "    clear_output()\n",
    "\n",
    "    # média dos retornos dos últimos 1000 episódios\n",
    "    return mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-01 00:07:34,642] Trial 19 finished with value: -0.0002299999999999961 and parameters: {'learning_rate': 0.00042264165874822634, 'clip_range': 0.4084153184980618}. Best is trial 12 with value: 0.003289999999999954.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PARAMETERS: {'learning_rate': 0.00038629218695293544, 'clip_range': 0.48591957734571395}\n",
      "BEST MODEL: ppo_train_2023-08-31 13:20:23.720199\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize',\n",
    "                        storage='sqlite:///resultado_optuna.db',\n",
    "                        study_name='ppo_slime_volley',\n",
    "                        load_if_exists=True)\n",
    "\n",
    "# maximiza o valor de retorno de train_exp_sarsa_continuous, rodando \"n_trials\" vezes\n",
    "study.optimize(optimize_ppo, n_trials=10)\n",
    "\n",
    "print(\"BEST PARAMETERS:\", study.best_params)\n",
    "print(\"BEST MODEL:\", f'ppo_train_{study.best_trial.datetime_start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-03 12:20:52,534] Using an existing study with name 'ppo_slime_volley' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./models/ppo_train_2023-08-31T13:20:23/ppo_1000000_steps.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henriquesabino/miniconda3/lib/python3.11/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/henriquesabino/miniconda3/lib/python3.11/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/henriquesabino/miniconda3/lib/python3.11/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize',\n",
    "                        storage='sqlite:///resultado_optuna.db',\n",
    "                        study_name='ppo_slime_volley',\n",
    "                        load_if_exists=True)\n",
    "\n",
    "env_name = 'SlimeVolleyNoFrameskip-v0'\n",
    "env = gym.make(env_name)\n",
    "env = RenderWrapper(env)\n",
    "env = ResizeObservation(env, (84, 84))\n",
    "env = GrayScaleObservation(env, True)\n",
    "env = ImageToPyTorch(env)\n",
    "env = BufferWrapper(env, 4, np.uint8)\n",
    "\n",
    "best_trial_date = study.best_trial.datetime_start.isoformat(timespec='seconds')\n",
    "model = createOrLoadModel('ppo', env, 'ppo_', '_steps.zip', f'ppo_train_{best_trial_date}/')\n",
    "\n",
    "sum_rewards = 0\n",
    "for _ in range(1000):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, _ = env.step(int(action))\n",
    "        sum_rewards += reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
